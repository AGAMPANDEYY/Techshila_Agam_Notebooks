{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install langchain\n!pip install torch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install peft","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \nfrom transformers import AutoModelForCausalLM,AutoTokenizer\nfrom transformers import BitsAndBytesConfig\n\nfrom huggingface_hub import login\n\nlogin(token=\"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\")\n\ncompute_dtype = getattr(torch, \"float16\")\nbnb_config=BitsAndBytesConfig(\n      load_in_4_bit=True,\n       bnb_4bit_quant_type='nf4',\n       bnb_4bit_compute_dtype=compute_dtype,\n       bnb_4bit_use_double_quant=False,\n)\n\ndevice_map = {\"\": 0}\n\nbase_model_name='microsoft/phi-2'\n\nimport peft\nbase_model= AutoModelForCausalLM.from_pretrained(\n                                base_model_name,\n                                device_map=\"auto\",\n                                 quantization_config=bnb_config,\n                                 trust_remote_code=True,\n                                 use_auth_token=True\n                                )\n\ntokenizer=AutoTokenizer.from_pretrained(base_model_name, padding=True,add_bos_token=True, trust_remote_code=True, use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token\n\nfrom peft import PeftModel\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel_name=\"AgamP/phi-2-PEFT-e5\"\n\nfrom peft import PeftModel\n\nft_model = PeftModel.from_pretrained(base_model, \"AgamP/phi-2-PEFT-e5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchains.llm import HuggingfacePipeline\nfrom langchain import PromptTemplate, LLMChain","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=ft_model,\n    tokenizer=tokenizer,\n    max_length=256,\n    temperature=0.6,\n    top_p=0.95,\n    repetition_penalty=1.2\n)\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\npipe.model.config.pad_token_id = pipe.model.config.eos_token_id","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain import PromptTemplate, LLMChain\n\ndef prompt_generator(job_role,candidate_answer,question_generated_phi2,question_count)\n\n    job_position=job_role\n    \n    if question_count<6:\n    \n        template = f\"\"\"respond to the instruction below. behave like a chatbot \n        and ask questions to the user. \n        You are best interviewer and conducting an interview for the Job Role, {job_position}.\\n\n        You have asked the candidate the following question: {question_generated_phi2}\\n\n        The candidate has responded as follows:\\n\n        {candidate_answer}\\n\n        Please formulate a thoughtful follow-up question to probe deeper into the candidate's understanding and experience of the candidate's response in relation to the desired skills and knowledge for the {job_position} role.\\n\n        ### Instruction:\n        {instruction}\n        Answer:\"\"\"\n        \n    else:\n        template = f\"\"\"respond to the instruction below. behave like a chatbot \n        and ask questions to the user. \n        You are best interviewer and conducting an interview for the Job Role, {job_position}.\\n\n        Based on your previous discussion, you have understood candidate's response\\n\n        Please formulate a thoughtful summary of the candidate for the {job_position} role and decide if the canidate has passed the interview and how well has he performed\\n\n        ### Instruction:\n        {instruction}\n        Answer:\"\"\"\n    \n    question_count+=1\n    prompt = PromptTemplate(template=template, input_variables=[\"instruction\"])\n    return prompt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_answer=\"Hello I am Agam Pandey, I am applying for the role of Data Scientist at your company\"  ### make changes here Vaibhav ya Dev jo dekh raha\njob_role=\"Data Scientist\"\n\nquestion_count=0\n\nquestion_generated_phi2=\"\"\n\nprompt=prompt_generator(job_role,candidate_answer,question_generated_phi2)\n\nllm_chain = LLMChain(prompt=prompt,\n                     llm=local_llm\n                     ) \nquestion_generated_phi2=llm_chain.run(candidate_answer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Summary Generation once 5-6 questions have been asked**","metadata":{}}]}