{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8072103,"sourceType":"datasetVersion","datasetId":4763101},{"sourceId":8079440,"sourceType":"datasetVersion","datasetId":4768452}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":47.590297,"end_time":"2024-01-20T10:21:41.280535","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-20T10:20:53.690238","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.730601,"end_time":"2024-01-20T10:20:57.894357","exception":false,"start_time":"2024-01-20T10:20:57.163756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:47:01.626603Z","iopub.execute_input":"2024-04-10T04:47:01.626967Z","iopub.status.idle":"2024-04-10T04:47:03.404842Z","shell.execute_reply.started":"2024-04-10T04:47:01.626932Z","shell.execute_reply":"2024-04-10T04:47:03.403827Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl","metadata":{"papermill":{"duration":21.139912,"end_time":"2024-01-20T10:21:19.149213","exception":false,"start_time":"2024-01-20T10:20:58.009301","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:47:03.406187Z","iopub.execute_input":"2024-04-10T04:47:03.406712Z","iopub.status.idle":"2024-04-10T04:47:40.434926Z","shell.execute_reply.started":"2024-04-10T04:47:03.406681Z","shell.execute_reply":"2024-04-10T04:47:40.433667Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires scipy<1.12,>=1.4.1, but you have scipy 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n# disable Weights and Biases\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"papermill":{"duration":0.020442,"end_time":"2024-01-20T10:21:19.183228","exception":false,"start_time":"2024-01-20T10:21:19.162786","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:47:40.438149Z","iopub.execute_input":"2024-04-10T04:47:40.438543Z","iopub.status.idle":"2024-04-10T04:47:40.444178Z","shell.execute_reply.started":"2024-04-10T04:47:40.438507Z","shell.execute_reply":"2024-04-10T04:47:40.443002Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nfrom huggingface_hub import interpreter_login\n\ninterpreter_login()","metadata":{"papermill":{"duration":18.720742,"end_time":"2024-01-20T10:21:37.916969","exception":true,"start_time":"2024-01-20T10:21:19.196227","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:47:40.445524Z","iopub.execute_input":"2024-04-10T04:47:40.445889Z","iopub.status.idle":"2024-04-10T04:48:25.023665Z","shell.execute_reply.started":"2024-04-10T04:47:40.445845Z","shell.execute_reply":"2024-04-10T04:48:25.022406Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-04-10 04:47:57.915035: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-10 04:47:57.915195: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-10 04:47:58.184268: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  ·····································\nAdd token as git credential? (Y/n)  Y\n"},{"name":"stdout","text":"Token is valid (permission: write).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from pynvml import *\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:48:25.025177Z","iopub.execute_input":"2024-04-10T04:48:25.028212Z","iopub.status.idle":"2024-04-10T04:48:25.035992Z","shell.execute_reply.started":"2024-04-10T04:48:25.028176Z","shell.execute_reply":"2024-04-10T04:48:25.034739Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<a name='2'></a>\n#### 2. Loading dataset","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# https://huggingface.co/datasets/neil-code/dialogsum-test\nhuggingface_dataset_name = \"AgamP/techshila_ml\"\ndataset = load_dataset(huggingface_dataset_name)\ndataset","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:48:25.037591Z","iopub.execute_input":"2024-04-10T04:48:25.038314Z","iopub.status.idle":"2024-04-10T04:48:27.189270Z","shell.execute_reply.started":"2024-04-10T04:48:25.038273Z","shell.execute_reply":"2024-04-10T04:48:27.188185Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Downloading data: 100%|██████████| 1.10M/1.10M [00:00<00:00, 3.74MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eac568444184c1986e7654b3f679606"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Unnamed: 0', 'Job Position', 'Question', 'Answer', 'Interview Phase', 'Answer Quality'],\n        num_rows: 2323\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset=dataset['train'].train_test_split(test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:48:27.190802Z","iopub.execute_input":"2024-04-10T04:48:27.191325Z","iopub.status.idle":"2024-04-10T04:48:27.214773Z","shell.execute_reply.started":"2024-04-10T04:48:27.191285Z","shell.execute_reply":"2024-04-10T04:48:27.213704Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"This is what the data looks like:","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:48:27.216180Z","iopub.execute_input":"2024-04-10T04:48:27.216639Z","iopub.status.idle":"2024-04-10T04:48:27.228618Z","shell.execute_reply.started":"2024-04-10T04:48:27.216595Z","shell.execute_reply":"2024-04-10T04:48:27.227475Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Unnamed: 0', 'Job Position', 'Question', 'Answer', 'Interview Phase', 'Answer Quality'],\n        num_rows: 1626\n    })\n    test: Dataset({\n        features: ['Unnamed: 0', 'Job Position', 'Question', 'Answer', 'Interview Phase', 'Answer Quality'],\n        num_rows: 697\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T05:12:12.433861Z","iopub.execute_input":"2024-04-10T05:12:12.434273Z","iopub.status.idle":"2024-04-10T05:12:12.443531Z","shell.execute_reply.started":"2024-04-10T05:12:12.434241Z","shell.execute_reply":"2024-04-10T05:12:12.442377Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"{'Unnamed: 0': 2037,\n 'Job Position': 'Nurse',\n 'Question': 'what would you do if a patient complains about their pain not being managed effectively',\n 'Answer': \"The primary step is to empathetically listen to the patient and affirm their experience. A comprehensive pain evaluation would be performed to comprehend the intensity and nature of their discomfort. This information would be shared with the patient's medical team to revisit and potentially revise their pain control plan. It's crucial to keep the patient informed about any modifications and engage them in the decisionmaking process fostering a sense of mutual trust and control.\",\n 'Interview Phase': 'Communication',\n 'Answer Quality': 'Unrated'}"},"metadata":{}}]},{"cell_type":"markdown","source":"<a name='3'></a>\n#### 3. Create bitsandbytes configuration\n\n**To load the model, we need a configuration class that specifies how we want the quantization to be performed. We’ll achieve this with BitesAndBytesConfig from the Transformers library. This will allow us to load our LLM in 4 bits. This way, we can divide the used memory by 4 and import the model on smaller devices.**","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\ndevice_map = {\"\": 0}","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:48:27.257024Z","iopub.execute_input":"2024-04-10T04:48:27.257449Z","iopub.status.idle":"2024-04-10T04:48:27.266960Z","shell.execute_reply.started":"2024-04-10T04:48:27.257416Z","shell.execute_reply":"2024-04-10T04:48:27.265945Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"<a name='4'></a>\n#### 4. Load Base Model\nLet's now load Phi-2 using 4-bit quantization!","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2024-04-10T04:48:27.268368Z","iopub.execute_input":"2024-04-10T04:48:27.268765Z","iopub.status.idle":"2024-04-10T04:48:41.824226Z","shell.execute_reply.started":"2024-04-10T04:48:27.268730Z","shell.execute_reply":"2024-04-10T04:48:41.823002Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name='microsoft/phi-2'\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name, \n                                                      device_map=device_map,\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:48:41.825835Z","iopub.execute_input":"2024-04-10T04:48:41.826183Z","iopub.status.idle":"2024-04-10T04:49:27.492433Z","shell.execute_reply.started":"2024-04-10T04:48:41.826149Z","shell.execute_reply":"2024-04-10T04:49:27.491044Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/863 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bd5cacbd29740558ad3a0cf623c33a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi.py:   0%|          | 0.00/9.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32775a7aec454e73b89d1cb0d38d4137"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n- configuration_phi.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi.py:   0%|          | 0.00/62.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e89309c005e46b6a42b3e95ea9fad9b"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n- modeling_phi.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9136722c99e4421a64a382438c61174"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cef96d9427a440668c06dcd415659b57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"946b7153ca4147cea81811958978a785"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34642c3cbe2b48c2b28618b48a97caf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e831666bdf1144ada2f75947806a2ca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7d37790275c48e18bf97339432d719b"}},"metadata":{}}]},{"cell_type":"markdown","source":"<a name='5'></a>\n#### 5. Tokenization\nSet up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\ntokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:49:27.495383Z","iopub.execute_input":"2024-04-10T04:49:27.495831Z","iopub.status.idle":"2024-04-10T04:49:29.868541Z","shell.execute_reply.started":"2024-04-10T04:49:27.495790Z","shell.execute_reply":"2024-04-10T04:49:29.867399Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29f0ce0a1d454dde9a3e6cd4ae8619b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a023c8dc126544429629dcb510923f9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b2aca197ae943d19cb4678c42550f03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43ab9b15fbf44d4b902783f2d1fbc036"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ca19cefb407478cb59a0afafc913f02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55f9661f04ee4804b9ba5b809d8e53a1"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token\n\ndef gen(model,p, maxlen=1000, sample=True):\n    toks = eval_tokenizer(p, return_tensors=\"pt\")\n    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:49:29.870435Z","iopub.execute_input":"2024-04-10T04:49:29.870852Z","iopub.status.idle":"2024-04-10T04:49:30.051731Z","shell.execute_reply.started":"2024-04-10T04:49:29.870814Z","shell.execute_reply":"2024-04-10T04:49:30.050516Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='6'></a>\n#### 6. Test the Model with Zero Shot Inferencing","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nseed = 42\nset_seed(seed)\n\nindex = 10\n\nprompt = dataset['train'][index]['Question']\nsummary = dataset['train'][index]['Answer']\njob_role=dataset['train'][index]['Job Position']\ninterview_phase=dataset['train'][index]['Interview Phase']\n\nformatted_prompt = f\"Instruct: You are an excellent interviewer for the given job role {job_role} and your task is to ask relevant questions to the candidate similar to {prompt}\\nOutput:\\n\"\nres = gen(original_model,formatted_prompt,100,)\n#print(res[0])\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'Answer:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:49:30.053006Z","iopub.execute_input":"2024-04-10T04:49:30.053321Z","iopub.status.idle":"2024-04-10T04:49:43.712296Z","shell.execute_reply.started":"2024-04-10T04:49:30.053295Z","shell.execute_reply":"2024-04-10T04:49:43.711216Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: You are an excellent interviewer for the given job role Nurse and your task is to ask relevant questions to the candidate similar to how do you stay up to date on medical advancements and procedures\nOutput:\n\n---------------------------------------------------------------------------------------------------\nAnswer:\nI stay current by reading medical journals attending conferences and participating in online learning modules. I also network with colleagues and discuss new developments with physicians and nurses.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n1. How do you stay up to date on the latest medical advancements and procedures?\n2. Can you describe a time when you had to quickly learn a new medical procedure?\n3. How do you ensure that you are providing the best possible care to your patients?\n4. How do you handle difficult or stressful situations in a medical setting?\n5. How do you stay organized and manage your time effectively in a fast-paced environment?\n6. Can you describe a time when\nCPU times: user 8.69 s, sys: 288 ms, total: 8.98 s\nWall time: 10.4 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='7'></a>\n#### 7. Pre-processing dataset","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"def create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output')\n    Then concatenate them using two newline characters \n    :param sample: Sample dictionnary\n    \"\"\"\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruct: You are an excellent interviewer for Job Roles mentioned by the candidate, Your task is to evaluate the answers by the candidate and generate questions relevant to the Job Role while keeping in mind the interview phase i.e. General, Technical, Behavioural,etc\"\n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n    \n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\"\n    input_context = f\"{sample['Job Position']}\"+f\"{sample['Interview Phase']}\"+f\"{sample['Question']}\" if sample[\"Question\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['Answer']}\"\n    end = f\"{END_KEY}\"\n    \n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    formatted_prompt = \"\\n\\n\".join(parts)\n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:49:43.713711Z","iopub.execute_input":"2024-04-10T04:49:43.714042Z","iopub.status.idle":"2024-04-10T04:49:43.722286Z","shell.execute_reply.started":"2024-04-10T04:49:43.714013Z","shell.execute_reply":"2024-04-10T04:49:43.721232Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length\n\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:49:43.723700Z","iopub.execute_input":"2024-04-10T04:49:43.723986Z","iopub.status.idle":"2024-04-10T04:49:43.737181Z","shell.execute_reply.started":"2024-04-10T04:49:43.723961Z","shell.execute_reply":"2024-04-10T04:49:43.736046Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from functools import partial\n\n# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    # Add prompt to each sample\n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)#, batched=True)\n    \n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        #remove_columns=['id', 'topic', 'dialogue', 'summary'],\n    )\n\n    # Filter out samples that have input_ids exceeding max_length\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n    \n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:50:13.168157Z","iopub.execute_input":"2024-04-10T04:50:13.169170Z","iopub.status.idle":"2024-04-10T04:50:13.176145Z","shell.execute_reply.started":"2024-04-10T04:50:13.169117Z","shell.execute_reply":"2024-04-10T04:50:13.175158Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# ## Pre-process dataset\nmax_length = get_max_length(original_model)\nprint(max_length)\n\ntrain_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\neval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['test'])","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:50:13.551031Z","iopub.execute_input":"2024-04-10T04:50:13.551981Z","iopub.status.idle":"2024-04-10T04:50:25.317833Z","shell.execute_reply.started":"2024-04-10T04:50:13.551945Z","shell.execute_reply":"2024-04-10T04:50:25.316789Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Found max lenth: 2048\n2048\nPreprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1626 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73b6454aa971464fb99221461c6c9882"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1626 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d58c8d8f800e4f4ab45a20a817c982f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1626 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82fc3f3f9a9a421cb26bbad64716a28d"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/697 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebd88167bed64d62bfbc2fcb21583e15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/697 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e63f3f37903f4916ad499d2e0500ce7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/697 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"056bed323d4b420aac0fd62b5a2093ec"}},"metadata":{}}]},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\n#print(train_dataset)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:50:25.319930Z","iopub.execute_input":"2024-04-10T04:50:25.320599Z","iopub.status.idle":"2024-04-10T04:50:25.325729Z","shell.execute_reply.started":"2024-04-10T04:50:25.320544Z","shell.execute_reply":"2024-04-10T04:50:25.324716Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (1626, 9)\nValidation: (697, 9)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='8'></a>\n#### 8. Setup the PEFT/LoRA model for Fine-Tuning\nNow, let's perform Parameter Efficient Fine-Tuning (PEFT) fine-tuning. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning.\nPEFT is a generic term that includes Low-Rank Adaptation (LoRA) and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, in essence, enables efficient model fine-tuning using fewer computational resources, often achievable with just a single GPU. Following LoRA fine-tuning for a specific task or use case, the outcome is an unchanged original LLM and the emergence of a considerably smaller \"LoRA adapter,\" often representing a single-digit percentage of the original LLM size (in MBs rather than GBs).\n\nDuring inference, the LoRA adapter must be combined with its original LLM. The advantage lies in the ability of many LoRA adapters to reuse the original LLM, thereby reducing overall memory requirements when handling multiple tasks and use cases.\n\nNote the rank (r) hyper-parameter, which defines the rank/dimension of the adapter to be trained.\nr is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n\nalpha is the scaling factor for the learned weights. The weight matrix is scaled by alpha/r, and thus a higher value for alpha assigns more weight to the LoRA activations.","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:50:25.327043Z","iopub.execute_input":"2024-04-10T04:50:25.327405Z","iopub.status.idle":"2024-04-10T04:50:25.345986Z","shell.execute_reply.started":"2024-04-10T04:50:25.327372Z","shell.execute_reply":"2024-04-10T04:50:25.345144Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"trainable model parameters: 262364160\nall model parameters: 1521392640\npercentage of trainable model parameters: 17.24%\n","output_type":"stream"}]},{"cell_type":"code","source":"print(original_model)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:50:25.347883Z","iopub.execute_input":"2024-04-10T04:50:25.348156Z","iopub.status.idle":"2024-04-10T04:50:25.363301Z","shell.execute_reply.started":"2024-04-10T04:50:25.348133Z","shell.execute_reply":"2024-04-10T04:50:25.362410Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"PhiForCausalLM(\n  (model): PhiModel(\n    (embed_tokens): Embedding(51200, 2560)\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0-31): 32 x PhiDecoderLayer(\n        (self_attn): PhiAttention(\n          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (rotary_emb): PhiRotaryEmbedding()\n        )\n        (mlp): PhiMLP(\n          (activation_fn): NewGELUActivation()\n          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n        )\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nconfig = LoraConfig(\n    r=32, #Rank\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        'k_proj',\n        'v_proj',\n        'dense'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\n# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\noriginal_model.gradient_checkpointing_enable()\n\n# 2 - Using the prepare_model_for_kbit_training method from PEFT\noriginal_model = prepare_model_for_kbit_training(original_model)\n\npeft_model = get_peft_model(original_model, config)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:50:25.364403Z","iopub.execute_input":"2024-04-10T04:50:25.364678Z","iopub.status.idle":"2024-04-10T04:50:25.887824Z","shell.execute_reply.started":"2024-04-10T04:50:25.364654Z","shell.execute_reply":"2024-04-10T04:50:25.886929Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Once everything is set up and the base model is prepared, we can use the print_trainable_parameters() helper function to see how many trainable parameters are in the model.","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:50:25.889191Z","iopub.execute_input":"2024-04-10T04:50:25.889619Z","iopub.status.idle":"2024-04-10T04:50:25.902458Z","shell.execute_reply.started":"2024-04-10T04:50:25.889584Z","shell.execute_reply":"2024-04-10T04:50:25.901368Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"trainable model parameters: 20971520\nall model parameters: 1542364160\npercentage of trainable model parameters: 1.36%\n","output_type":"stream"}]},{"cell_type":"code","source":"# See how the model looks different now, with the LoRA adapters added:\nprint(peft_model)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:50:25.903719Z","iopub.execute_input":"2024-04-10T04:50:25.904027Z","iopub.status.idle":"2024-04-10T04:50:25.925876Z","shell.execute_reply.started":"2024-04-10T04:50:25.904001Z","shell.execute_reply":"2024-04-10T04:50:25.924911Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): PhiForCausalLM(\n      (model): PhiModel(\n        (embed_tokens): Embedding(51200, 2560)\n        (embed_dropout): Dropout(p=0.0, inplace=False)\n        (layers): ModuleList(\n          (0-31): 32 x PhiDecoderLayer(\n            (self_attn): PhiAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (dense): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2560, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2560, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): PhiRotaryEmbedding()\n            )\n            (mlp): PhiMLP(\n              (activation_fn): NewGELUActivation()\n              (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n            )\n            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n      )\n      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='9'></a>\n#### 9. Train PEFT Adapter\n\nDefine training arguments and create Trainer instance.","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"output_dir = './peft-dialogue-summary-training/final-checkpoint'\nimport transformers\n\npeft_training_args = TrainingArguments(\n    output_dir = output_dir,\n    warmup_steps=1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=5,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=1,\n    logging_dir=\"./logs\",\n    save_strategy=\"steps\",\n    save_steps=1,\n    evaluation_strategy=\"steps\",\n    eval_steps=1,\n    do_eval=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    overwrite_output_dir = 'True',\n    group_by_length=True,\n)\n\npeft_model.config.use_cache = False\n\npeft_trainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=peft_training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:51:02.580660Z","iopub.execute_input":"2024-04-10T04:51:02.581689Z","iopub.status.idle":"2024-04-10T04:51:02.627649Z","shell.execute_reply.started":"2024-04-10T04:51:02.581647Z","shell.execute_reply":"2024-04-10T04:51:02.626274Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"peft_training_args.device","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:51:04.418854Z","iopub.execute_input":"2024-04-10T04:51:04.419251Z","iopub.status.idle":"2024-04-10T04:51:04.425577Z","shell.execute_reply.started":"2024-04-10T04:51:04.419219Z","shell.execute_reply":"2024-04-10T04:51:04.424577Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"peft_trainer.train()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T04:51:05.650039Z","iopub.execute_input":"2024-04-10T04:51:05.650733Z","iopub.status.idle":"2024-04-10T05:02:53.217454Z","shell.execute_reply.started":"2024-04-10T04:51:05.650697Z","shell.execute_reply":"2024-04-10T05:02:53.216336Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 11:35, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.191000</td>\n      <td>2.935906</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.445700</td>\n      <td>2.847509</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.629500</td>\n      <td>2.763322</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.564500</td>\n      <td>2.710619</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.647700</td>\n      <td>2.679134</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5, training_loss=2.495686435699463, metrics={'train_runtime': 706.4528, 'train_samples_per_second': 0.057, 'train_steps_per_second': 0.007, 'total_flos': 182343221821440.0, 'train_loss': 2.495686435699463, 'epoch': 0.02})"},"metadata":{}}]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T05:02:53.228038Z","iopub.status.idle":"2024-04-10T05:02:53.228594Z","shell.execute_reply.started":"2024-04-10T05:02:53.228291Z","shell.execute_reply":"2024-04-10T05:02:53.228313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Saving the trained adapter","metadata":{}},{"cell_type":"code","source":"import os \nos.chdir(r'/kaggle/working')\nfrom IPython.display import FileLink \nFileLink(r'peft-dialogue-summary-training')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pushing model on huggingface repo","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import interpreter_login\n\ninterpreter_login()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:09:18.903367Z","iopub.execute_input":"2024-04-10T05:09:18.904280Z","iopub.status.idle":"2024-04-10T05:09:30.133680Z","shell.execute_reply.started":"2024-04-10T05:09:18.904233Z","shell.execute_reply":"2024-04-10T05:09:30.132568Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n    Setting a new token will erase the existing one.\n    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  ·····································\nAdd token as git credential? (Y/n)  Y\n"},{"name":"stdout","text":"Token is valid (permission: write).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import create_repo\ncreate_repo(\"phi-2-PEFT-e5\")","metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:10:32.437101Z","iopub.execute_input":"2024-04-10T05:10:32.437917Z","iopub.status.idle":"2024-04-10T05:10:33.715077Z","shell.execute_reply.started":"2024-04-10T05:10:32.437884Z","shell.execute_reply":"2024-04-10T05:10:33.714051Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"RepoUrl('https://huggingface.co/AgamP/phi-2-PEFT-e5', endpoint='https://huggingface.co', repo_type='model', repo_id='AgamP/phi-2-PEFT-e5')"},"metadata":{}}]},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\napi=HfApi()\n\napi.upload_folder(\n    folder_path=\"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-5\",\n    repo_id=\"AgamP/phi-2-PEFT-e5\",\n    repo_type=\"model\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:15:57.959463Z","iopub.execute_input":"2024-04-10T05:15:57.959898Z","iopub.status.idle":"2024-04-10T05:16:02.869223Z","shell.execute_reply.started":"2024-04-10T05:15:57.959865Z","shell.execute_reply":"2024-04-10T05:16:02.868177Z"},"trusted":true},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50ac264c074f4e8f8236e2e14e7f36ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0e632e5aec0424f88e4c67955da6a3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"optimizer.pt:   0%|          | 0.00/42.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7202ee0223d24acb903b9545cc1ae777"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fef357ca7a3a4ac2b582992b835f0bce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5226bb52ef0b4c7dacf58e72b0eb5cfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7b7e9a0cab444b78f1eaeb930b8e302"}},"metadata":{}},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/AgamP/phi-2-PEFT-e5/commit/e73e4a074da18c3872e44cd0dc852a159119af0d', commit_message='Upload folder using huggingface_hub', commit_description='', oid='e73e4a074da18c3872e44cd0dc852a159119af0d', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"# Free memory for merging weights\ndel original_model\ndel peft_trainer\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.execute_input":"2024-01-20T10:12:15.717375Z","iopub.status.busy":"2024-01-20T10:12:15.716634Z","iopub.status.idle":"2024-01-20T10:12:16.006683Z","shell.execute_reply":"2024-01-20T10:12:16.005744Z","shell.execute_reply.started":"2024-01-20T10:12:15.717341Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.execute_input":"2024-01-20T10:12:19.432308Z","iopub.status.busy":"2024-01-20T10:12:19.431935Z","iopub.status.idle":"2024-01-20T10:12:19.437916Z","shell.execute_reply":"2024-01-20T10:12:19.436995Z","shell.execute_reply.started":"2024-01-20T10:12:19.432277Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='10'></a>\n#### 10. Evaluate the Model Qualitatively (Human Evaluation)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nbase_model_id = \"microsoft/phi-2\"\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T05:25:21.084490Z","iopub.execute_input":"2024-04-10T05:25:21.085311Z","iopub.status.idle":"2024-04-10T05:25:25.112640Z","shell.execute_reply.started":"2024-04-10T05:25:21.085273Z","shell.execute_reply":"2024-04-10T05:25:25.111695Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0bec419fe2047a19525eb3b9affdf97"}},"metadata":{}}]},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\neval_tokenizer.pad_token = eval_tokenizer.eos_token","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T05:25:30.026528Z","iopub.execute_input":"2024-04-10T05:25:30.026931Z","iopub.status.idle":"2024-04-10T05:25:30.215303Z","shell.execute_reply.started":"2024-04-10T05:25:30.026901Z","shell.execute_reply":"2024-04-10T05:25:30.214408Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import PeftModel\n\nft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-5\",torch_dtype=torch.float16,is_trainable=False)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T05:25:40.852510Z","iopub.execute_input":"2024-04-10T05:25:40.853528Z","iopub.status.idle":"2024-04-10T05:25:41.480010Z","shell.execute_reply.started":"2024-04-10T05:25:40.853481Z","shell.execute_reply":"2024-04-10T05:25:41.479034Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"%time\nfrom transformers import set_seed\nseed = 42\nset_seed(seed)\n\nindex = 1000\n\nprompt = dataset['train'][index]['Question']\nsummary = dataset['train'][index]['Answer']\njob_role=dataset['train'][index]['Job Position']\ninterview_phase=dataset['train'][index]['Interview Phase']\n\nformatted_prompt = f\"Instruct: You are an excellent interviewer for the given job role {job_role} and your task is to ask relevant questions to the candidate similar to {prompt}\\nOutput:\\n\"\nres = gen(original_model,formatted_prompt,100,)\n#print(res[0])\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'Answer:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T05:28:05.266806Z","iopub.execute_input":"2024-04-10T05:28:05.267237Z","iopub.status.idle":"2024-04-10T05:28:05.446096Z","shell.execute_reply.started":"2024-04-10T05:28:05.267204Z","shell.execute_reply":"2024-04-10T05:28:05.444951Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 4 µs, sys: 2 µs, total: 6 µs\nWall time: 11.9 µs\n---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: You are an excellent interviewer for the given job role Data Scientist and your task is to ask relevant questions to the candidate similar to Describe your experience with model explainability techniques and their importance in machine learning projects.\nOutput:\n\n---------------------------------------------------------------------------------------------------\nAnswer:\nI have experience with model explainability techniques such as feature importance analysis, partial dependence plots, SHAP values, and LIME (Local Interpretable Model-agnostic Explanations). These techniques help understand the contribution of features to model predictions, identify patterns and correlations, and explain the decision-making process of complex machine learning models to stakeholders. Model explainability enhances transparency, trust, and accountability in machine learning projects, enabling stakeholders to understand, validate, and interpret model outputs.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='11'></a>\n#### 11. Evaluate the Model Quantitatively (with ROUGE Metric)\nPerform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time). ","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T05:28:31.187560Z","iopub.execute_input":"2024-04-10T05:28:31.187939Z","iopub.status.idle":"2024-04-10T05:28:35.308392Z","shell.execute_reply.started":"2024-04-10T05:28:31.187911Z","shell.execute_reply":"2024-04-10T05:28:35.307065Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"300d3aec042e49379abb2de23bbdab72"}},"metadata":{}}]},{"cell_type":"code","source":"dataset[]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=pd.read_csv(\"/kaggle/input/combined-data-techshila/data (1).csv\")\ndf1.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T06:15:44.985134Z","iopub.execute_input":"2024-04-10T06:15:44.985684Z","iopub.status.idle":"2024-04-10T06:15:45.020575Z","shell.execute_reply.started":"2024-04-10T06:15:44.985650Z","shell.execute_reply":"2024-04-10T06:15:45.019516Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                     Job Position  \\\n0           0  Customer Service Representative   \n1           1  Customer Service Representative   \n2           2  Customer Service Representative   \n3           3  Customer Service Representative   \n4           4  Customer Service Representative   \n\n                                            Question  \\\n0                What are Your Biggest Achievements?   \n1  Name any Two Improvements You Made in the Prev...   \n2  Tell me about a professional accomplishment yo...   \n3  Have you ever utilized customer feedback to en...   \n4  Have You Used Customer Feedback to Ensure Busi...   \n\n                                              Answer Interview Phase  \\\n0  During my last job, I¬†learned¬†some interpers...         General   \n1  As a few of my team members were late to work,...         General   \n2  One of my proudest professional accomplishment...         General   \n3  Yes, I've leveraged customer feedback to impro...   Role Specific   \n4  Yes, I have used customer feedback to improve ...   Role Specific   \n\n  Answer Quality  \n0        Average  \n1           Good  \n2           Good  \n3        Average  \n4           Good  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Job Position</th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>Interview Phase</th>\n      <th>Answer Quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Customer Service Representative</td>\n      <td>What are Your Biggest Achievements?</td>\n      <td>During my last job, I¬†learned¬†some interpers...</td>\n      <td>General</td>\n      <td>Average</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Customer Service Representative</td>\n      <td>Name any Two Improvements You Made in the Prev...</td>\n      <td>As a few of my team members were late to work,...</td>\n      <td>General</td>\n      <td>Good</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Customer Service Representative</td>\n      <td>Tell me about a professional accomplishment yo...</td>\n      <td>One of my proudest professional accomplishment...</td>\n      <td>General</td>\n      <td>Good</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Customer Service Representative</td>\n      <td>Have you ever utilized customer feedback to en...</td>\n      <td>Yes, I've leveraged customer feedback to impro...</td>\n      <td>Role Specific</td>\n      <td>Average</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Customer Service Representative</td>\n      <td>Have You Used Customer Feedback to Ensure Busi...</td>\n      <td>Yes, I have used customer feedback to improve ...</td>\n      <td>Role Specific</td>\n      <td>Good</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"questions = dataset['test'][595:600]['Question']\njob_position=dataset['test'][595:600]['Job Position']\ninterview_phase=dataset['test'][595:600]['Interview Phase']\nanswer_quality=dataset['test'][595:600]['Answer Quality']\nanswer = dataset['test'][595:600]['Answer']","metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:59:52.937530Z","iopub.execute_input":"2024-04-10T05:59:52.938470Z","iopub.status.idle":"2024-04-10T05:59:52.947886Z","shell.execute_reply.started":"2024-04-10T05:59:52.938431Z","shell.execute_reply":"2024-04-10T05:59:52.946847Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"questions","metadata":{"execution":{"iopub.status.busy":"2024-04-10T05:59:57.508803Z","iopub.execute_input":"2024-04-10T05:59:57.509200Z","iopub.status.idle":"2024-04-10T05:59:57.516085Z","shell.execute_reply.started":"2024-04-10T05:59:57.509172Z","shell.execute_reply":"2024-04-10T05:59:57.514967Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"['why did you become a nursing assistant',\n 'what would you do if you had a difficult patient',\n 'how would you deal with a rude patient',\n 'tell us about a time when you had to do something difficult how did you handle that and how can that help you as a Certified nursing assistants',\n 'Can you discuss the challenges of working with big data and techniques to handle scalability, performance, and resource constraints?']"},"metadata":{}}]},{"cell_type":"code","source":"ground_truth_questions=df1.loc[df1['Job Position'].isin(job_position),['Question','Job Position']]\nground_truth_questions","metadata":{"execution":{"iopub.status.busy":"2024-04-10T06:19:50.427464Z","iopub.execute_input":"2024-04-10T06:19:50.428234Z","iopub.status.idle":"2024-04-10T06:19:50.455638Z","shell.execute_reply.started":"2024-04-10T06:19:50.428198Z","shell.execute_reply":"2024-04-10T06:19:50.454367Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"                                               Question    Job Position\n790           why did you decide on a career as a nurse           Nurse\n791           why did you decide on a career as a nurse           Nurse\n792           why did you decide on a career as a nurse           Nurse\n793           why did you decide on a career as a nurse           Nurse\n794           why did you decide on a career as a nurse           Nurse\n...                                                 ...             ...\n2247  Describe your experience with deep learning ar...  Data Scientist\n2248  How do you approach building interpretable mac...  Data Scientist\n2249  Can you explain the concept of feature selecti...  Data Scientist\n2250  Describe your experience with Bayesian statist...  Data Scientist\n2251  How do you approach building predictive models...  Data Scientist\n\n[1232 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Job Position</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>790</th>\n      <td>why did you decide on a career as a nurse</td>\n      <td>Nurse</td>\n    </tr>\n    <tr>\n      <th>791</th>\n      <td>why did you decide on a career as a nurse</td>\n      <td>Nurse</td>\n    </tr>\n    <tr>\n      <th>792</th>\n      <td>why did you decide on a career as a nurse</td>\n      <td>Nurse</td>\n    </tr>\n    <tr>\n      <th>793</th>\n      <td>why did you decide on a career as a nurse</td>\n      <td>Nurse</td>\n    </tr>\n    <tr>\n      <th>794</th>\n      <td>why did you decide on a career as a nurse</td>\n      <td>Nurse</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2247</th>\n      <td>Describe your experience with deep learning ar...</td>\n      <td>Data Scientist</td>\n    </tr>\n    <tr>\n      <th>2248</th>\n      <td>How do you approach building interpretable mac...</td>\n      <td>Data Scientist</td>\n    </tr>\n    <tr>\n      <th>2249</th>\n      <td>Can you explain the concept of feature selecti...</td>\n      <td>Data Scientist</td>\n    </tr>\n    <tr>\n      <th>2250</th>\n      <td>Describe your experience with Bayesian statist...</td>\n      <td>Data Scientist</td>\n    </tr>\n    <tr>\n      <th>2251</th>\n      <td>How do you approach building predictive models...</td>\n      <td>Data Scientist</td>\n    </tr>\n  </tbody>\n</table>\n<p>1232 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"ground_truth1=ground_truth_questions.loc[ground_truth_questions['Job Position']==job_position[1],'Question']\nground_truth1","metadata":{"execution":{"iopub.status.busy":"2024-04-10T06:29:22.000345Z","iopub.execute_input":"2024-04-10T06:29:22.000819Z","iopub.status.idle":"2024-04-10T06:29:22.011865Z","shell.execute_reply.started":"2024-04-10T06:29:22.000791Z","shell.execute_reply":"2024-04-10T06:29:22.010576Z"},"trusted":true},"execution_count":90,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"790             why did you decide on a career as a nurse\n791             why did you decide on a career as a nurse\n792             why did you decide on a career as a nurse\n793             why did you decide on a career as a nurse\n794             why did you decide on a career as a nurse\n                              ...                        \n2047    what techniques would you use to calm an upset...\n2048    can you recall an episode at work where you ma...\n2049    in what ways has your prior work history equip...\n2050    can you describe a situation where you had to ...\n2051    how would you handle a situation where you dis...\nName: Question, Length: 653, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\noriginal_model_response = []\nground_truth_response = []\npeft_model_response = []\n\n\n\nfor idx, question in enumerate(questions):\n    #human_baseline_text_output = human_baseline_summaries[idx]\n    ground_truth=ground_truth_questions.loc[ground_truth_questions['Job Position']==job_position[idx],'Question']\n    prompt = f\"Instruct: You are an excellent interviewer for the given job role {job_position[idx]} and your task is to ask relevant questions to the candidate like {questions[idx]} but not exactly same.You have to also rate the answer of the candidate like {answer[idx]} like {answer_quality[idx]} and give a summary \\nOutput:\\n\"\n    \n    original_model_res = gen(original_model,prompt,500,)\n    original_model_text_output = original_model_res[0].split('Output:')[1]\n    \n    peft_model_res = gen(ft_model,prompt,100,)\n    peft_model_output = peft_model_res[0].split('Output:')[1]\n    print(peft_model_output)\n    peft_model_text_output, success, result = peft_model_output.partition('#End')\n    \n\n    original_model_response.append(original_model_text_output)\n    peft_model_response.append(peft_model_text_output)\n    ground_truth_response.append(ground_truth)\n\nzipped_response = list(zip(ground_truth_response,original_model_response, peft_model_response))\n \ndf = pd.DataFrame(zipped_response, columns = ['ground_truth_response','original_model_response', 'peft_model_response'])\ndf","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T06:25:30.202455Z","iopub.execute_input":"2024-04-10T06:25:30.203212Z","iopub.status.idle":"2024-04-10T06:28:07.865552Z","shell.execute_reply.started":"2024-04-10T06:25:30.203178Z","shell.execute_reply":"2024-04-10T06:28:07.864310Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n1. Can you tell me about your experience as a nursing assistant?\n2. What motivated you to become a nursing assistant?\n3. How do you handle difficult situations with patients or colleagues?\n4. Can you give an example of a time when you went above and beyond to provide quality care for a patient?\n5. How do you stay up-to-date with the latest medical advancements and practices?\n6. How do you prioritize tasks and manage your time effectively as a\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n1. Can you tell me about a time when you had to handle a difficult patient?\n2. How did you manage the situation?\n3. What steps did you take to ensure that patient care remained the top priority?\n4. Did you seek guidance from supervisors or colleagues?\n5. How did you address the patient's concerns?\n6. How did you handle any emotional escalation during the situation?\n7. Can you give an example of a time when you had to handle\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nInterviewer: Can you tell me about a time when you had to deal with a difficult patient?\nCandidate: Yes, I had a patient who was very rude and disrespectful towards me and the other staff members. I tried to remain calm and professional, but it was challenging.\nInterviewer: How did you handle the situation?\nCandidate: I tried to understand the patient's perspective and address their concerns. I also communicated clearly and respectfully with them, and tried to find a solution\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n1. Can you tell me about a time when you had to do something difficult?\n2. How did you handle that situation?\n3. Can you give an example of a time when you had to mediate a conflict within your team?\n4. How did you facilitate open communication during the conflict?\n5. How did you mediate discussions to find a collaborative solution?\n6. How can this experience help you as a Certified nursing assistant?\n7. How do you prioritize\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n1. Can you discuss the challenges of working with big data and techniques to handle scalability, performance, and resource constraints?\n2. How do you ensure data quality and consistency when working with big data?\n3. What are some common data integration challenges when working with big data?\n4. Can you explain the concept of data partitioning and how it helps with big data processing?\n5. How do you handle data privacy and security concerns when working with big data?\n6.\n","output_type":"stream"},{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"                               ground_truth_response  \\\n0  790             why did you decide on a career...   \n1  790             why did you decide on a career...   \n2  832      why would you be the best candidate t...   \n3  790             why did you decide on a career...   \n4  2127    What programming languages are you pro...   \n\n                             original_model_response  \\\n0  \\n1. Why did you choose to become a nursing as...   \n1  \\n1. Can you tell me about a time when you had...   \n2  \\nInterviewer: Can you tell me about a time wh...   \n3  \\n1. Can you tell me about a time when you had...   \n4  \\n1. Can you discuss the challenges of working...   \n\n                                 peft_model_response  \n0  \\n1. Can you tell me about your experience as ...  \n1  \\n1. Can you tell me about a time when you had...  \n2  \\nInterviewer: Can you tell me about a time wh...  \n3  \\n1. Can you tell me about a time when you had...  \n4  \\n1. Can you discuss the challenges of working...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ground_truth_response</th>\n      <th>original_model_response</th>\n      <th>peft_model_response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>790             why did you decide on a career...</td>\n      <td>\\n1. Why did you choose to become a nursing as...</td>\n      <td>\\n1. Can you tell me about your experience as ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>790             why did you decide on a career...</td>\n      <td>\\n1. Can you tell me about a time when you had...</td>\n      <td>\\n1. Can you tell me about a time when you had...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>832      why would you be the best candidate t...</td>\n      <td>\\nInterviewer: Can you tell me about a time wh...</td>\n      <td>\\nInterviewer: Can you tell me about a time wh...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>790             why did you decide on a career...</td>\n      <td>\\n1. Can you tell me about a time when you had...</td>\n      <td>\\n1. Can you tell me about a time when you had...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2127    What programming languages are you pro...</td>\n      <td>\\n1. Can you discuss the challenges of working...</td>\n      <td>\\n1. Can you discuss the challenges of working...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-10T06:05:00.876658Z","iopub.execute_input":"2024-04-10T06:05:00.877767Z","iopub.status.idle":"2024-04-10T06:05:00.886228Z","shell.execute_reply.started":"2024-04-10T06:05:00.877729Z","shell.execute_reply":"2024-04-10T06:05:00.885121Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"original_model_response    \\n1. Can you tell me about your experience as ...\npeft_model_response        \\n1. Why did you choose to become a nursing as...\nName: 0, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T05:55:16.183596Z","iopub.execute_input":"2024-04-10T05:55:16.184069Z","iopub.status.idle":"2024-04-10T05:55:33.653356Z","shell.execute_reply.started":"2024-04-10T05:55:16.183990Z","shell.execute_reply":"2024-04-10T05:55:33.651925Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c333764a4ef5ebbe3b66bf2ca8f34a7728f382a47e946ef764606600afcd1f90\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_response,\n    references=ground_truth_response[0:len(original_model_response)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_response,\n    references=ground_truth_response[0:len(peft_model_response)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T06:31:16.104392Z","iopub.execute_input":"2024-04-10T06:31:16.104855Z","iopub.status.idle":"2024-04-10T06:32:06.228976Z","shell.execute_reply.started":"2024-04-10T06:31:16.104825Z","shell.execute_reply":"2024-04-10T06:32:06.227708Z"},"trusted":true},"execution_count":91,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34538224fa38458abcb43831a869d4c3"}},"metadata":{}},{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.20385520948369024, 'rouge2': 0.12328301886792455, 'rougeL': 0.171788411818597, 'rougeLsum': 0.1744837053002191}\nPEFT MODEL:\n{'rouge1': 0.38164692030318637, 'rouge2': 0.2662982871791761, 'rougeL': 0.3400653430626621, 'rougeLsum': 0.34043424152252316}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-04-10T06:32:32.600123Z","iopub.execute_input":"2024-04-10T06:32:32.600939Z","iopub.status.idle":"2024-04-10T06:32:32.608057Z","shell.execute_reply.started":"2024-04-10T06:32:32.600900Z","shell.execute_reply":"2024-04-10T06:32:32.606843Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 17.78%\nrouge2: 14.30%\nrougeL: 16.83%\nrougeLsum: 16.60%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}